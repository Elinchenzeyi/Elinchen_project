[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Elinchen_project",
    "section": "",
    "text": "1 Topic\nComparing LIME with Intrinsically Interpretable Models\n\n\n2 Research Question:\nDo LIME’s explanations about black box models align with the interpretability of intrinsically interpretable models like linear regression?\n\n\n3 Key elements to examine:\n\nDo the most important features identified by LIME match those from linear regression?\nHow consistent are LIME’s explanations across similar observations?\nWhen they disagree, what patterns emerge?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Topic</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "2  Data",
    "section": "",
    "text": "2.1 For my project, I used the Boston Housing dataset to do the analysis.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#missing-value-analysis",
    "href": "data.html#missing-value-analysis",
    "title": "2  Data",
    "section": "3.2 Missing value analysis",
    "text": "3.2 Missing value analysis",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "3  Results",
    "section": "",
    "text": "Code\nlibrary(ggplot2)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "data.html#description",
    "href": "data.html#description",
    "title": "2  Data",
    "section": "3.1 Description",
    "text": "3.1 Description",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "d3graph.html",
    "href": "d3graph.html",
    "title": "4  Interactive graph",
    "section": "",
    "text": "#LIME with Random Forest Model\n\n4.0.1 I created the graph using the LIME algorithm for 10 individual predictions made by a Random Forest model on the Boston Housing dataset. In each facet, it shows a single observation. and for each observation, the top five most influential features are presented.\n\n\n4.0.2 Green bars are the features that increase the predicted housing price, and red bars are the features that decrease the predicted housing price.\n\n\nCode\nlibrary(MASS)\ndata(\"Boston\")\nlm_model &lt;- lm(medv ~ ., data = Boston)\nsummary(lm_model)\n\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# Fit linear regression model\nlm_model &lt;- lm(medv ~ ., data = Boston)\nsummary(lm_model)\n\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# Extract and rank absolute coefficients\ncoefficients_df &lt;- as.data.frame(summary(lm_model)$coefficients)\ncoefficients_df$Feature &lt;- rownames(coefficients_df)\ncoefficients_df &lt;- coefficients_df[-1, ]  # Remove intercept\ncoefficients_df$AbsEstimate &lt;- abs(coefficients_df$Estimate)\ncoefficients_df &lt;- coefficients_df[order(-coefficients_df$AbsEstimate), ]\n\nhead(coefficients_df, 5)  # Top 5 features\n\n\n           Estimate Std. Error   t value     Pr(&gt;|t|) Feature AbsEstimate\nnox     -17.7666112  3.8197437 -4.651257 4.245644e-06     nox  17.7666112\nrm        3.8098652  0.4179253  9.116140 1.979441e-18      rm   3.8098652\nchas      2.6867338  0.8615798  3.118381 1.925030e-03    chas   2.6867338\ndis      -1.4755668  0.1994547 -7.398004 6.013491e-13     dis   1.4755668\nptratio  -0.9527472  0.1308268 -7.282511 1.308835e-12 ptratio   0.9527472\n\n\nCode\nlibrary(randomForest)\n\n\nrandomForest 4.7-1.2\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\nCode\nlibrary(lime)\nlibrary(caret)\n\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'ggplot2'\n\n\nThe following object is masked from 'package:randomForest':\n\n    margin\n\n\nLoading required package: lattice\n\n\nCode\nlibrary(ggplot2)\nset.seed(42)\n\ntrain_index &lt;- createDataPartition(Boston$medv, p = 0.8, list = FALSE)\ntrain_data &lt;- Boston[train_index, ]\ntest_data &lt;- Boston[-train_index, ]\n\nrf_model &lt;- randomForest(medv ~ ., data = train_data)\nrf_model\n\n\n\nCall:\n randomForest(formula = medv ~ ., data = train_data) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 4\n\n          Mean of squared residuals: 11.55371\n                    % Var explained: 85.85\n\n\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:lime':\n\n    explain\n\n\nThe following object is masked from 'package:randomForest':\n\n    combine\n\n\nThe following object is masked from 'package:MASS':\n\n    select\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n✔ readr     2.1.5     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::combine()  masks randomForest::combine()\n✖ dplyr::explain()  masks lime::explain()\n✖ dplyr::filter()   masks stats::filter()\n✖ dplyr::lag()      masks stats::lag()\n✖ purrr::lift()     masks caret::lift()\n✖ ggplot2::margin() masks randomForest::margin()\n✖ dplyr::select()   masks MASS::select()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(caret)\nlibrary(lime)\nlibrary(randomForest)\nlibrary(MASS)\n\nmodel_type.randomForest &lt;- function(x, ...) {\n  \"regression\"\n}\n\n# Define how to generate predictions\npredict_model.randomForest &lt;- function(x, newdata, type, ...) {\n  data.frame(Response = predict(x, newdata = newdata))\n}\n\nexplainer &lt;- lime::lime(\n  x = train_data[, -which(names(train_data) == \"medv\")],\n  model = rf_model\n)\n\n\nWarning: chas does not contain enough variance to use quantile binning. Using\nstandard binning instead.\n\n\nCode\nexplanations &lt;- lime::explain(\n  x = test_data[1:10, -which(names(test_data) == \"medv\")],\n  explainer = explainer,\n  n_features = 5\n)\n\n\nWarning in gower_work(x = x, y = y, pair_x = pair_x, pair_y = pair_y, n = NULL,\n: skipping variable with zero or non-finite range\n\n\nWarning in gower_work(x = x, y = y, pair_x = pair_x, pair_y = pair_y, n = NULL,\n: skipping variable with zero or non-finite range\nWarning in gower_work(x = x, y = y, pair_x = pair_x, pair_y = pair_y, n = NULL,\n: skipping variable with zero or non-finite range\nWarning in gower_work(x = x, y = y, pair_x = pair_x, pair_y = pair_y, n = NULL,\n: skipping variable with zero or non-finite range\nWarning in gower_work(x = x, y = y, pair_x = pair_x, pair_y = pair_y, n = NULL,\n: skipping variable with zero or non-finite range\nWarning in gower_work(x = x, y = y, pair_x = pair_x, pair_y = pair_y, n = NULL,\n: skipping variable with zero or non-finite range\nWarning in gower_work(x = x, y = y, pair_x = pair_x, pair_y = pair_y, n = NULL,\n: skipping variable with zero or non-finite range\nWarning in gower_work(x = x, y = y, pair_x = pair_x, pair_y = pair_y, n = NULL,\n: skipping variable with zero or non-finite range\nWarning in gower_work(x = x, y = y, pair_x = pair_x, pair_y = pair_y, n = NULL,\n: skipping variable with zero or non-finite range\nWarning in gower_work(x = x, y = y, pair_x = pair_x, pair_y = pair_y, n = NULL,\n: skipping variable with zero or non-finite range\n\n\nCode\n# Pick top 5 features per case\ntop_explanations &lt;- explanations %&gt;%\n  group_by(case) %&gt;%\n  top_n(5, wt = abs(feature_weight)) %&gt;%\n  ungroup()\n\n# Clean factor levels for proper ordering\ntop_explanations &lt;- top_explanations %&gt;%\n  mutate(feature = factor(feature, levels = unique(feature[order(abs(feature_weight))])))\n\n# Custom LIME Plot\nggplot(top_explanations, aes(x = reorder(feature, feature_weight), y = feature_weight, fill = feature_weight &gt; 0)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  facet_wrap(~ case, scales = \"free_y\") +\n  labs(\n    title = \"LIME Explanations for Boston Housing Predictions\",\n    x = \"Feature\",\n    y = \"Contribution to Prediction\"\n  ) +\n  scale_fill_manual(values = c(\"red\", \"green\")) +\n  theme_minimal(base_size = 12)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interactive graph</span>"
    ]
  },
  {
    "objectID": "data.html#for-my-project-i-used-the-boston-housing-dataset-to-do-the-analysis.",
    "href": "data.html#for-my-project-i-used-the-boston-housing-dataset-to-do-the-analysis.",
    "title": "2  Data",
    "section": "",
    "text": "2.1.1 The following is the code using the random forest model for the Boston Housing Dataset!:\n\n\nCode\nlibrary(MASS)\nlibrary(randomForest)\n\n\nrandomForest 4.7-1.2\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\nCode\nlibrary(lime)\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::combine()  masks randomForest::combine()\n✖ dplyr::explain()  masks lime::explain()\n✖ dplyr::filter()   masks stats::filter()\n✖ dplyr::lag()      masks stats::lag()\n✖ ggplot2::margin() masks randomForest::margin()\n✖ dplyr::select()   masks MASS::select()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(caret)\n\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\nCode\ndata(\"Boston\")\nX &lt;- Boston %&gt;% select(-medv)\ny &lt;- Boston$medv\n\nset.seed(42)\ntrain_index &lt;- createDataPartition(y, p = 0.8, list = FALSE)\nX_train &lt;- X[train_index, ]\ny_train &lt;- y[train_index]\nX_test &lt;- X[-train_index, ]\ny_test &lt;- y[-train_index]\n\nrf_model &lt;- randomForest(X_train, y_train)\nrf_model\n\n\n\nCall:\n randomForest(x = X_train, y = y_train) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 4\n\n          Mean of squared residuals: 11.55371\n                    % Var explained: 85.85\n\n\n\n\n2.1.2 Here is code using the linear regression model for the Boston Housing Dataset!:\n\n\nCode\nlm_model &lt;- lm(y_train ~ ., data = X_train)\nlm_model\n\n\n\nCall:\nlm(formula = y_train ~ ., data = X_train)\n\nCoefficients:\n(Intercept)         crim           zn        indus         chas          nox  \n  33.998764    -0.122347     0.043296     0.003389     1.554178   -15.555219  \n         rm          age          dis          rad          tax      ptratio  \n   3.859758     0.004788    -1.305775     0.319482    -0.012468    -0.953531  \n      black        lstat  \n   0.009516    -0.516037  \n\n\nCode\nlm_importance &lt;- abs(coef(lm_model)[-1])\nnames(lm_importance) &lt;- names(X_train)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#i-will-use-both-the-random-forest-model-and-linear-regression-model-to-generate-the-feature-importance-explanations.",
    "href": "data.html#i-will-use-both-the-random-forest-model-and-linear-regression-model-to-generate-the-feature-importance-explanations.",
    "title": "2  Data",
    "section": "2.2 I will use both the random forest model and linear regression model to generate the feature importance explanations.",
    "text": "2.2 I will use both the random forest model and linear regression model to generate the feature importance explanations.\nThe code part provides the Random Forest model for the Boston Housing Dataset! 😊\nHere is code 1:\n\n\nCode\nlibrary(MASS)\nlibrary(randomForest)\n\n\nrandomForest 4.7-1.2\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\nCode\nlibrary(lime)\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::combine()  masks randomForest::combine()\n✖ dplyr::explain()  masks lime::explain()\n✖ dplyr::filter()   masks stats::filter()\n✖ dplyr::lag()      masks stats::lag()\n✖ ggplot2::margin() masks randomForest::margin()\n✖ dplyr::select()   masks MASS::select()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(caret)\n\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\nCode\ndata(\"Boston\")\nX &lt;- Boston %&gt;% select(-medv)\ny &lt;- Boston$medv\n\nset.seed(42)\ntrain_index &lt;- createDataPartition(y, p = 0.8, list = FALSE)\nX_train &lt;- X[train_index, ]\ny_train &lt;- y[train_index]\nX_test &lt;- X[-train_index, ]\ny_test &lt;- y[-train_index]\n\nrf_model &lt;- randomForest(X_train, y_train)\nrf_model\n\n\n\nCall:\n randomForest(x = X_train, y = y_train) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 4\n\n          Mean of squared residuals: 11.55371\n                    % Var explained: 85.85\n\n\n\n\nCode\nlm_model &lt;- lm(y_train ~ ., data = X_train)\nlm_model\n\n\n\nCall:\nlm(formula = y_train ~ ., data = X_train)\n\nCoefficients:\n(Intercept)         crim           zn        indus         chas          nox  \n  33.998764    -0.122347     0.043296     0.003389     1.554178   -15.555219  \n         rm          age          dis          rad          tax      ptratio  \n   3.859758     0.004788    -1.305775     0.319482    -0.012468    -0.953531  \n      black        lstat  \n   0.009516    -0.516037",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  }
]