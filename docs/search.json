[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Elinchen_project",
    "section": "",
    "text": "1 Topic\nComparing LIME with Intrinsically Interpretable Models\n\n\n2 Research Question:\nDo LIMEâ€™s explanations about black box models align with the interpretability of intrinsically interpretable models like linear regression?\n\n\n3 Key elements to examine:\n\nDo the most important features identified by LIME match those from linear regression?\nHow consistent are LIMEâ€™s explanations across similar observations?\nWhen they disagree, what patterns emerge?",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Topic</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "2Â  Data",
    "section": "",
    "text": "2.1 For my project, I used the Boston Housing dataset to do the analysis.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#missing-value-analysis",
    "href": "data.html#missing-value-analysis",
    "title": "2Â  Data",
    "section": "3.2 Missing value analysis",
    "text": "3.2 Missing value analysis",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "3Â  Results",
    "section": "",
    "text": "Code\nlibrary(ggplot2)",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "data.html#description",
    "href": "data.html#description",
    "title": "2Â  Data",
    "section": "3.1 Description",
    "text": "3.1 Description",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "d3graph.html",
    "href": "d3graph.html",
    "title": "4Â  Interactive graph",
    "section": "",
    "text": "The following graph is the force plot that is based on using SHAP model. In this plot, it shows the variables that affect the predictions the most. Specifically, I can notice that based on the predictions, rm is 6.01, which falls into the red region that shows decrease 0.225. I think it might suggest that average number of rooms puts the prediction down by 0.225. However, the per capita crime rate, pupil-teacher ratio, lower status population, proposition of old housing are the variables that increases our prediction. Specifcially, I can notice that per capital crime rate is 0.0883, which increases our model by 0.103. pupil teacher ratio is 15.2, which increases our model by 0.378. Age is 66.6, which increases our model by 0.587.\nE[f(x)] gives the base value across the training data.\n\n\nCode\nlibrary(fastshap)\nlibrary(shapviz)\nlibrary(ggplot2)\nlibrary(MASS)\nlibrary(randomForest)\n\n\nrandomForest 4.7-1.2\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\n\nCode\nlibrary(lime)\n\n\n\nAttaching package: 'lime'\n\n\nThe following object is masked from 'package:fastshap':\n\n    explain\n\n\nCode\nlibrary(tidyverse)\n\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.4     âœ” readr     2.1.5\nâœ” forcats   1.0.0     âœ” stringr   1.5.1\nâœ” lubridate 1.9.3     âœ” tibble    3.2.1\nâœ” purrr     1.0.2     âœ” tidyr     1.3.1\n\n\nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::combine()       masks randomForest::combine()\nâœ– dplyr::explain()       masks lime::explain(), fastshap::explain()\nâœ– dplyr::filter()        masks stats::filter()\nâœ– dplyr::lag()           masks stats::lag()\nâœ– randomForest::margin() masks ggplot2::margin()\nâœ– dplyr::select()        masks MASS::select()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(caret)\n\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\nCode\ndata(\"Boston\")\nX &lt;- Boston %&gt;% select(-medv)\ny &lt;- Boston$medv\n\n\n\n\nCode\ndata(\"Boston\")\n\nset.seed(42)\ntrain_index &lt;- createDataPartition(y, p = 0.8, list = FALSE)\nX_train &lt;- X[train_index, ]\ny_train &lt;- y[train_index]\nX_test &lt;- X[-train_index, ]\ny_test &lt;- y[-train_index]\n\ntrain_idx &lt;- sample(1:nrow(Boston), 0.8 * nrow(Boston))\ntrain_data &lt;- Boston[train_idx, ]\ntest_data &lt;- Boston[-train_idx, ]\n\nrf_model &lt;- randomForest(X_train, y_train)\nrf_model\n\n\n\nCall:\n randomForest(x = X_train, y = y_train) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 4\n\n          Mean of squared residuals: 11.26525\n                    % Var explained: 86.21\n\n\nCode\nlm_model &lt;- lm(y_train ~ ., data = X_train)\nlm_model\n\n\n\nCall:\nlm(formula = y_train ~ ., data = X_train)\n\nCoefficients:\n(Intercept)         crim           zn        indus         chas          nox  \n  33.998764    -0.122347     0.043296     0.003389     1.554178   -15.555219  \n         rm          age          dis          rad          tax      ptratio  \n   3.859758     0.004788    -1.305775     0.319482    -0.012468    -0.953531  \n      black        lstat  \n   0.009516    -0.516037  \n\n\nCode\nlm_importance &lt;- abs(coef(lm_model)[-1])\nnames(lm_importance) &lt;- names(X_train)\n\nrf_model &lt;- randomForest(medv ~ ., data = train_data)\n\nlm_model &lt;- lm(medv ~ ., data = train_data)\n\npredict_model_rf &lt;- function(model, newdata) {\n  predict(model, newdata)\n}\n\n\n\n\nCode\npredict_rf &lt;- function(model, newdata) {\n  predict(model, newdata = newdata)\n}\n\nshap_values &lt;- fastshap::explain(\n  rf_model,\n  X = test_data[1:5, -which(names(test_data) == \"medv\")],  \n  pred_wrapper = predict_rf,\n  nsim = 100\n)\n\ndim(shap_values) \n\n\n[1]  5 13\n\n\nCode\ndim(test_data[1:5, -which(names(test_data) == \"medv\")]) \n\n\n[1]  5 13\n\n\nCode\ncases_to_plot &lt;- 1:5\nshp &lt;- shapviz(\n  object = shap_values[cases_to_plot, ], \n  X = test_data[cases_to_plot, -which(names(test_data) == \"medv\")]\n)\n\n\nsv_force(shp, row_id = 1) \n\n\n\n\n\n\n\n\n\nThen, the modelâ€™s actual prediction is the formula: Base Value + Sum of SHAP Contributions = 0 + (+0.378 + 0.528 + 0.587 - 0.225 + â€¦) =1.3\n\n\nCode\nfunction_x&lt;-1.3\n\n\nIt shows the variables that are the most important ones n our SHAP feature graph and from this graph with all the yellow bars, we can notice that lstat is the most influential one followed by age, and then pupil-teacher ratio.\n\n\nCode\nsv_importance(shp)+\n   labs(title = \"Boston Housing: SHAP Feature Importance\",\n       x = \"Impact on Home Value Price\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nGraphs with linear regression models:\n\n\nCode\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(broom)  # For tidy model summaries\n\n# Load data\ndata(Boston)\n\n# Train-test split (consistent with your RF model)\nset.seed(42)\ntrain_idx &lt;- sample(1:nrow(Boston), 0.8 * nrow(Boston))\ntrain_data &lt;- Boston[train_idx, ]\ntest_data &lt;- Boston[-train_idx, ]\n\n# Fit linear regression\nlm_model &lt;- lm(medv ~ ., data = train_data)  # medv = median home value ($1000s)\n\nlm_coef &lt;- tidy(lm_model) %&gt;%\n  mutate(\n    abs_effect = abs(estimate),  # Absolute effect size\n    significance = ifelse(p.value &lt; 0.05, \"Significant\", \"Not significant\")\n  )\n\n# Print top 5 most impactful features\nlm_coef %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  arrange(desc(abs_effect)) %&gt;%\n  head(5)\n\n\n# A tibble: 5 Ã— 7\n  term    estimate std.error statistic  p.value abs_effect significance   \n  &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;          \n1 nox      -17.2       4.26      -4.03 6.77e- 5     17.2   Significant    \n2 rm         3.54      0.454      7.80 5.85e-14      3.54  Significant    \n3 chas       1.95      0.997      1.95 5.19e- 2      1.95  Not significant\n4 dis       -1.43      0.230     -6.24 1.17e- 9      1.43  Significant    \n5 ptratio   -0.984     0.148     -6.64 1.08e-10      0.984 Significant    \n\n\n\n\nCode\nggplot(lm_coef %&gt;% filter(term != \"(Intercept)\"), \n       aes(x = reorder(term, estimate), y = estimate, fill = significance)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(\n    title = \"Linear Regression: Feature Effects on Home Value\",\n    x = \"Feature\",\n    y = \"Coefficient Estimate ($1000s)\",\n    fill = \"Significance\"\n  ) +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Interactive graph</span>"
    ]
  },
  {
    "objectID": "data.html#for-my-project-i-used-the-boston-housing-dataset-to-do-the-analysis.",
    "href": "data.html#for-my-project-i-used-the-boston-housing-dataset-to-do-the-analysis.",
    "title": "2Â  Data",
    "section": "",
    "text": "2.1.1 The following is the code using the random forest model for the Boston Housing Dataset!:\n\n\nCode\nlibrary(MASS)\nlibrary(randomForest)\n\n\nrandomForest 4.7-1.2\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\nCode\nlibrary(lime)\nlibrary(tidyverse)\n\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.4     âœ” readr     2.1.5\nâœ” forcats   1.0.0     âœ” stringr   1.5.1\nâœ” ggplot2   3.5.1     âœ” tibble    3.2.1\nâœ” lubridate 1.9.3     âœ” tidyr     1.3.1\nâœ” purrr     1.0.2     \n\n\nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::combine()  masks randomForest::combine()\nâœ– dplyr::explain()  masks lime::explain()\nâœ– dplyr::filter()   masks stats::filter()\nâœ– dplyr::lag()      masks stats::lag()\nâœ– ggplot2::margin() masks randomForest::margin()\nâœ– dplyr::select()   masks MASS::select()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(caret)\n\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\nCode\ndata(\"Boston\")\nX &lt;- Boston %&gt;% select(-medv)\ny &lt;- Boston$medv\n\nset.seed(42)\ntrain_index &lt;- createDataPartition(y, p = 0.8, list = FALSE)\nX_train &lt;- X[train_index, ]\ny_train &lt;- y[train_index]\nX_test &lt;- X[-train_index, ]\ny_test &lt;- y[-train_index]\n\nrf_model &lt;- randomForest(X_train, y_train)\nrf_model\n\n\n\nCall:\n randomForest(x = X_train, y = y_train) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 4\n\n          Mean of squared residuals: 11.55371\n                    % Var explained: 85.85\n\n\n\n\n2.1.2 Here is code using the linear regression model for the Boston Housing Dataset!:\n\n\nCode\nlm_model &lt;- lm(y_train ~ ., data = X_train)\nlm_model\n\n\n\nCall:\nlm(formula = y_train ~ ., data = X_train)\n\nCoefficients:\n(Intercept)         crim           zn        indus         chas          nox  \n  33.998764    -0.122347     0.043296     0.003389     1.554178   -15.555219  \n         rm          age          dis          rad          tax      ptratio  \n   3.859758     0.004788    -1.305775     0.319482    -0.012468    -0.953531  \n      black        lstat  \n   0.009516    -0.516037  \n\n\nCode\nlm_importance &lt;- abs(coef(lm_model)[-1])\nnames(lm_importance) &lt;- names(X_train)",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#i-will-use-both-the-random-forest-model-and-linear-regression-model-to-generate-the-feature-importance-explanations.",
    "href": "data.html#i-will-use-both-the-random-forest-model-and-linear-regression-model-to-generate-the-feature-importance-explanations.",
    "title": "2Â  Data",
    "section": "2.2 I will use both the random forest model and linear regression model to generate the feature importance explanations.",
    "text": "2.2 I will use both the random forest model and linear regression model to generate the feature importance explanations.\nThe code part provides the Random Forest model for the Boston Housing Dataset! ðŸ˜Š\nHere is code 1:\n\n\nCode\nlibrary(MASS)\nlibrary(randomForest)\n\n\nrandomForest 4.7-1.2\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\nCode\nlibrary(lime)\nlibrary(tidyverse)\n\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.4     âœ” readr     2.1.5\nâœ” forcats   1.0.0     âœ” stringr   1.5.1\nâœ” ggplot2   3.5.1     âœ” tibble    3.2.1\nâœ” lubridate 1.9.3     âœ” tidyr     1.3.1\nâœ” purrr     1.0.2     \n\n\nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::combine()  masks randomForest::combine()\nâœ– dplyr::explain()  masks lime::explain()\nâœ– dplyr::filter()   masks stats::filter()\nâœ– dplyr::lag()      masks stats::lag()\nâœ– ggplot2::margin() masks randomForest::margin()\nâœ– dplyr::select()   masks MASS::select()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(caret)\n\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\nCode\ndata(\"Boston\")\nX &lt;- Boston %&gt;% select(-medv)\ny &lt;- Boston$medv\n\nset.seed(42)\ntrain_index &lt;- createDataPartition(y, p = 0.8, list = FALSE)\nX_train &lt;- X[train_index, ]\ny_train &lt;- y[train_index]\nX_test &lt;- X[-train_index, ]\ny_test &lt;- y[-train_index]\n\nrf_model &lt;- randomForest(X_train, y_train)\nrf_model\n\n\n\nCall:\n randomForest(x = X_train, y = y_train) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 4\n\n          Mean of squared residuals: 11.55371\n                    % Var explained: 85.85\n\n\n\n\nCode\nlm_model &lt;- lm(y_train ~ ., data = X_train)\nlm_model\n\n\n\nCall:\nlm(formula = y_train ~ ., data = X_train)\n\nCoefficients:\n(Intercept)         crim           zn        indus         chas          nox  \n  33.998764    -0.122347     0.043296     0.003389     1.554178   -15.555219  \n         rm          age          dis          rad          tax      ptratio  \n   3.859758     0.004788    -1.305775     0.319482    -0.012468    -0.953531  \n      black        lstat  \n   0.009516    -0.516037",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Data</span>"
    ]
  }
]