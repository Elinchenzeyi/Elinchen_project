[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Elinchen_project",
    "section": "",
    "text": "1 Topic\nComparing SHAP with Intrinsically Interpretable Models",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Topic</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "2  Data",
    "section": "",
    "text": "2.1 For my project, I used the Boston Housing dataset to do the analysis.\nThese are the libraries that we need to use for our Boston Housing Data:\nMASS-for Boston dataset\nDplyr-for data wrangling\nRandomForest-for random forest model\nggplot2-for plotting\nshapvz-for using the SHAP model\ntidyr-for reshaping my data\ntibble-for conversion\ncorrr-for correlation\nCode\nlibrary(MASS)           \nlibrary(dplyr)         \n\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:MASS':\n\n    select\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(randomForest)   \n\n\nrandomForest 4.7-1.2\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nCode\nlibrary(ggplot2)        \n\n\n\nAttaching package: 'ggplot2'\n\n\nThe following object is masked from 'package:randomForest':\n\n    margin\n\n\nCode\nlibrary(shapviz)       \nlibrary(tidyr)          \nlibrary(tibble)         \nlibrary(corrr)\nThe Boston Housing Dataset:\nI set X and Y. For X, it shows all the features in the dataset For Y, it shows the target in our dataset, which is medv\nCode\nlibrary(MASS)\nlibrary(randomForest)\nlibrary(lime)\n\n\n\nAttaching package: 'lime'\n\n\nThe following object is masked from 'package:dplyr':\n\n    explain\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ lubridate 1.9.3     ✔ stringr   1.5.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ randomForest::combine() masks dplyr::combine()\n✖ lime::explain()         masks dplyr::explain()\n✖ dplyr::filter()         masks stats::filter()\n✖ dplyr::lag()            masks stats::lag()\n✖ ggplot2::margin()       masks randomForest::margin()\n✖ dplyr::select()         masks MASS::select()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(caret)\n\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\nCode\ndata(\"Boston\")\nX &lt;- Boston %&gt;% select(-medv)\ny &lt;- Boston$medv\nHere, I made the train and test data:\nCode\nset.seed(42)\ntrain_index &lt;- createDataPartition(y, p = 0.8, list = FALSE)\nX_train &lt;- X[train_index, ]\ny_train &lt;- y[train_index]\nX_test &lt;- X[-train_index, ]\ny_test &lt;- y[-train_index]\nBased on the train and test data, I trained the random forest model:\nThe result shows that my model explains 85.85% variability in the dataset and the average squared difference between the actual and predicted values is 11.5\nBased on this result, I could know that the random forest model is a good fit for SHAP interpretation!\nCode\nrf_model &lt;- randomForest(X_train, y_train)\nrf_model\n\n\n\nCall:\n randomForest(x = X_train, y = y_train) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 4\n\n          Mean of squared residuals: 11.55371\n                    % Var explained: 85.85\nHere is the trained linear regression model:\nCode\nlm_model &lt;- lm(y_train ~ ., data = X_train)\nlm_model\n\n\n\nCall:\nlm(formula = y_train ~ ., data = X_train)\n\nCoefficients:\n(Intercept)         crim           zn        indus         chas          nox  \n  33.998764    -0.122347     0.043296     0.003389     1.554178   -15.555219  \n         rm          age          dis          rad          tax      ptratio  \n   3.859758     0.004788    -1.305775     0.319482    -0.012468    -0.953531  \n      black        lstat  \n   0.009516    -0.516037  \n\n\nCode\nlm_importance &lt;- abs(coef(lm_model)[-1])\nnames(lm_importance) &lt;- names(X_train)\nThen, I made the prediction for SHAP and computed the SHAP values. The SHAP values could tell that all the features have the standardized values, like they are the standardized observations.\nCode\nlibrary(MASS)\nlibrary(randomForest)\nlibrary(lime)\nlibrary(tidyverse)\n\ndata(Boston)\nset.seed(42)\ntrain_idx &lt;- sample(1:nrow(Boston), 0.8 * nrow(Boston))\ntrain_data &lt;- Boston[train_idx, ]\ntest_data &lt;- Boston[-train_idx, ]\n\nrf_model &lt;- randomForest(medv ~ ., data = train_data)\n\n\nlm_model &lt;- lm(medv ~ ., data = train_data)\n\npredict_model_rf &lt;- function(model, newdata) {\n  predict(model, newdata)\n}\n\n\npredict_rf &lt;- function(model, newdata) {\n  predict(model, newdata = newdata)\n}\n\nshap_values &lt;- fastshap::explain(\n  rf_model,\n  X = train_data[, -which(names(train_data) == \"medv\")],\n  pred_wrapper = predict_rf,\n  nsim = 10 \n)\n\nshap_values[1, c(\"lstat\", \"rm\", \"nox\", \"ptratio\", \"chas\")]\n\n\n    lstat        rm       nox   ptratio      chas \n-2.161482 -2.886117  1.371208  0.285852 -0.042688\nHere are all the features for the SHAP(random forest). (I just listed two rows for a basic description) I will explain more feature importance with the graphs in graph.qmd section!\nCode\ndim(shap_values) \n\n\n[1] 404  13\n\n\nCode\ndim(test_data[1:5, -which(names(test_data) == \"medv\")]) \n\n\n[1]  5 13\n\n\nCode\ncases_to_plot &lt;- 1:5\nshp &lt;- shapviz(\n  object = shap_values[cases_to_plot, ], \n  X = test_data[cases_to_plot, -which(names(test_data) == \"medv\")]\n)\n\nshp\n\n\n'shapviz' object representing 5 x 13 SHAP matrix. Top lines:\n\n         crim        zn       indus         chas       nox        rm        age\n49  0.4494302 -0.046122 -0.33126233 -0.042688000  1.371208 -2.886117 -0.5635007\n485 0.1589982 -0.087402  0.05185467 -0.007845667 -0.107147 -3.029262  0.2951956\n           dis         rad         tax    ptratio       black     lstat\n49  -0.8811324 -0.13644338  0.22308215  0.2858520 -0.01124322 -2.161482\n485  0.2343892 -0.05031733 -0.06796433 -0.3393103 -0.06831133 -3.055491\nCode\nlm_importance &lt;- coef(lm_model)[-1] %&gt;% abs() %&gt;% sort(decreasing = TRUE) %&gt;%\n  enframe(name = \"feature\", value = \"lm_importance\")\nlm_importance\n\n\n# A tibble: 13 × 2\n   feature lm_importance\n   &lt;chr&gt;           &lt;dbl&gt;\n 1 nox        17.2      \n 2 rm          3.54     \n 3 chas        1.95     \n 4 dis         1.43     \n 5 ptratio     0.984    \n 6 lstat       0.531    \n 7 rad         0.303    \n 8 crim        0.118    \n 9 zn          0.0353   \n10 tax         0.0111   \n11 black       0.00824  \n12 age         0.00327  \n13 indus       0.0000893\nHere are the feature importance that I could easily interpret from the linear regression model. It is obviously that nox is the most important feature followed by rm, chas, dis, and so forth.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#missing-value-analysis",
    "href": "data.html#missing-value-analysis",
    "title": "2  Data",
    "section": "3.2 Missing value analysis",
    "text": "3.2 Missing value analysis",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "3  Results (With Some Graphs)",
    "section": "",
    "text": "From the data section, I generated the train and test data, and the two different models are based on the data.\nI could tell that speaking of the model performance comparison:\nRandom Forest vs. Linear Regression\nMean Squared Residuals for random forest: 11.5.\nMean Squared Residuals for linear regression: tendency to be higher\n% Variance Explained for random forest: 85.85%\n% Variance Explained for linear regression: tendency to be lower\nLinearity Assumed for random forest? No\nLinearity Assumed for linear regression? Yes\nCaptures Interactions for random forest? Yes\nCaptures Interactions for linear regression? No\nCaptures Non-linearity for random forest? Yes\nCaptures Non-linearity for linear regression? No\n\n\nCode\nlibrary(randomForest)\n\n\nrandomForest 4.7-1.2\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\nCode\nlibrary(MASS)\nlibrary(randomForest)\nlibrary(lime)\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::combine()  masks randomForest::combine()\n✖ dplyr::explain()  masks lime::explain()\n✖ dplyr::filter()   masks stats::filter()\n✖ dplyr::lag()      masks stats::lag()\n✖ ggplot2::margin() masks randomForest::margin()\n✖ dplyr::select()   masks MASS::select()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(caret)\n\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\nCode\ndata(\"Boston\")\nX &lt;- Boston %&gt;% select(-medv)\ny &lt;- Boston$medv\n\nset.seed(42)\ntrain_index &lt;- createDataPartition(y, p = 0.8, list = FALSE)\nX_train &lt;- X[train_index, ]\ny_train &lt;- y[train_index]\nX_test &lt;- X[-train_index, ]\ny_test &lt;- y[-train_index]\n\nrf_model &lt;- randomForest(X_train, y_train, ntree = 500)\nlm_model &lt;- lm(medv ~ ., data = Boston[train_index, ])\nrf_model\n\n\n\nCall:\n randomForest(x = X_train, y = y_train, ntree = 500) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 4\n\n          Mean of squared residuals: 11.55371\n                    % Var explained: 85.85\n\n\nCode\nlm_model\n\n\n\nCall:\nlm(formula = medv ~ ., data = Boston[train_index, ])\n\nCoefficients:\n(Intercept)         crim           zn        indus         chas          nox  \n  33.998764    -0.122347     0.043296     0.003389     1.554178   -15.555219  \n         rm          age          dis          rad          tax      ptratio  \n   3.859758     0.004788    -1.305775     0.319482    -0.012468    -0.953531  \n      black        lstat  \n   0.009516    -0.516037  \n\n\n\n\nCode\nlibrary(fastshap)  # For fast SHAP approximations\n\n\n\nAttaching package: 'fastshap'\n\n\nThe following object is masked from 'package:dplyr':\n\n    explain\n\n\nThe following object is masked from 'package:lime':\n\n    explain\n\n\nCode\nlibrary(randomForest)\nlibrary(dplyr)\nlibrary(FNN)\nlibrary(ggplot2)\n\ndata(Boston)\nX &lt;- Boston %&gt;% select(-medv)\ny &lt;- Boston$medv\n\nset.seed(42)\ntrain_idx &lt;- sample(seq_len(nrow(X)), size = 0.8 * nrow(X))\nX_train &lt;- X[train_idx, ]\ny_train &lt;- y[train_idx]\nX_test &lt;- X[-train_idx, ]\ny_test &lt;- y[-train_idx]\n\n# Train the Random Forest model\nrf_model &lt;- randomForest(X_train, y_train, ntree = 500)\n\npred_fun &lt;- function(model, newdata) predict(model, newdata = newdata)\n\nshap_values &lt;- fastshap::explain(\n  rf_model,\n  X = X_train,\n  pred_wrapper = pred_fun,\n  nsim = 50\n)\n\nk &lt;- 5\nknn_indices &lt;- FNN::knnx.index(\n  data = X_train,\n  query = X_test,\n  k = k\n)\n\nshap_diffs &lt;- sapply(1:nrow(X_test), function(i) {\n  test_shap &lt;- shap_values[i, ]\n  knn_shap &lt;- colMeans(shap_values[knn_indices[i, ], ])  \n  abs(test_shap - knn_shap)  \n})\n\navg_shap_diffs &lt;- rowMeans(shap_diffs)\n\navg_shap_diffs\n\n\n      crim         zn      indus       chas        nox         rm        age \n0.51302044 0.05223589 0.34801011 0.05687939 0.86705348 2.51544185 0.37886528 \n       dis        rad        tax    ptratio      black      lstat \n0.63195177 0.09928622 0.30506053 0.65355903 0.28250744 3.63036893 \n\n\nAfter knowing about the model differences, it is essential to check for consistency in features for the SHAP random forest model for Boston housing dataset.\nHere, Knn equals to 5 is a way to understand the local consistency, like it helps me to measure how stable the features are compared to its 5 most similar neighbors.\nI could interpret from the data that the average SHAP differences is low for the features like: -crim, -zn, -indus, -chas, -age, -dis, -rad, -tax, -ptratio, -black.\nHowever, there is a high average differences for the features: -rm, -lstat, -nox\nThen, I made a PDP and ICE plot for knowing the reasons like why some features were not consistent:\n\n\nCode\nlibrary(pdp)\n\n\n\nAttaching package: 'pdp'\n\n\nThe following object is masked from 'package:purrr':\n\n    partial\n\n\nCode\nlibrary(ggplot2)\n\n# Partial Dependence Plot\npdp_lstat &lt;- partial(rf_model, pred.var = \"lstat\", train = X_train)\nggplot(pdp_lstat, aes(lstat, yhat)) + \n  geom_line() +\n  labs(title = \"Partial Dependence of 'lstat' on Price\")\n\n\n\n\n\n\n\n\n\nCode\n# ICE Plots\nice_lstat &lt;- partial(rf_model, pred.var = \"lstat\", train = X_train, ice = TRUE)\nggplot(ice_lstat, aes(lstat, yhat, group = yhat.id)) + \n  geom_line(alpha = 0.2) +\n  labs(title = \"ICE Plots for 'lstat'\")\n\n\n\n\n\n\n\n\n\nFrom the graphs, I could notice that the trend of the curves show similar pattern, which is that as the number of the lower percentage of the population increases, the house price decreases significantly.\nThe pdp is non-linear and drops significantly within the 0-10 range for lstat, which implies that the house price is highly sensitive to lstat.\nSince the high inconsistency and non-linearity appear for the feature lstat, I made the graph for the rm+ lstat for knowing if lstat has an interaction with rm that causes the SHAP’s explanation to be inconsistent.\n\n\nCode\nlibrary(fastshap)\nshap_values &lt;- explain(rf_model, X = X_train, nsim = 50, pred_wrapper = pred_fun)\n\nshap_values &lt;- fastshap::explain(\n  rf_model, \n  X = X_train, \n  pred_wrapper = pred_fun, \n  nsim = 50\n)\n\nshap_df &lt;- as.data.frame(shap_values)\nshap_df$lstat &lt;- X_train$lstat  \nshap_df$crim &lt;- X_train$crim    \n\nlibrary(ggplot2)\nggplot(shap_df, aes(lstat, lstat, color = crim)) +\n  geom_point(alpha = 0.5) +\n  scale_color_gradient(low = \"blue\", high = \"red\") +\n  labs(\n    title = \"SHAP Values for 'lstat' (Colored by Crime Rate)\",\n    x = \"lstat (Actual Value)\",\n    y = \"SHAP Value (Impact on Price)\"\n  )\n\n\n\n\n\n\n\n\n\nI think from the graph above, it shows that the lstat and crim together give a strong impact on the SHAP value (House price). As the population of the lower percentage of status increases and the number of crime goes higher, the impact on the house price increases. Therefore, it explains for why there is a high inconsistency for the lstat feature itself-there is a strong interaction! Similarly, I think the high inconsistency for nox and rm could also be influenced by the interaction effects.\nThen, I explore the feature significance in linear regression model, and notice that chas is the feature with no significance. It is because the p-value for chas is larger than the standard threshold, 0.05.\nBut again, I believe it is still an important feature even though it is not statistically significant. In fact, it is important as it might have an interaction effect or it might has the coefficient value.\n\n\nCode\nlibrary(fastshap)\nlibrary(shapviz)\nlibrary(ggplot2)\nlibrary(MASS)\nlibrary(randomForest)\nlibrary(lime)\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(broom)\ntrain_idx &lt;- sample(1:nrow(Boston), 0.8 * nrow(Boston))\ntrain_data &lt;- Boston[train_idx, ]\ntest_data &lt;- Boston[-train_idx, ]\n\npredict_rf &lt;- function(model, newdata) {\n  predict(model, newdata = newdata)\n}\n\nshap_values &lt;- fastshap::explain(\n  rf_model,\n  X = test_data[1:5, -which(names(test_data) == \"medv\")],  \n  pred_wrapper = predict_rf,\n  nsim = 100\n)\n\ndim(shap_values) \n\n\n[1]  5 13\n\n\nCode\ndim(test_data[1:5, -which(names(test_data) == \"medv\")]) \n\n\n[1]  5 13\n\n\nCode\ncases_to_plot &lt;- 1:5\nshp &lt;- shapviz(\n  object = shap_values[cases_to_plot, ], \n  X = test_data[cases_to_plot, -which(names(test_data) == \"medv\")]\n)\n\n\n\ndata(Boston)\n\n\nset.seed(42)\ntrain_idx &lt;- sample(1:nrow(Boston), 0.8 * nrow(Boston))\ntrain_data &lt;- Boston[train_idx, ]\ntest_data &lt;- Boston[-train_idx, ]\n\n\nlm_model &lt;- lm(medv ~ ., data = train_data)\n\nlm_coef &lt;- tidy(lm_model) %&gt;%\n  mutate(\n    abs_effect = abs(estimate), \n    significance = ifelse(p.value &lt; 0.05, \"Significant\", \"Not significant\")\n  )\n\n\nlm_coef %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  arrange(desc(abs_effect))\n\n\n# A tibble: 13 × 7\n   term       estimate std.error statistic  p.value abs_effect significance   \n   &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;          \n 1 nox     -17.2         4.26     -4.03    6.77e- 5 17.2       Significant    \n 2 rm        3.54        0.454     7.80    5.85e-14  3.54      Significant    \n 3 chas      1.95        0.997     1.95    5.19e- 2  1.95      Not significant\n 4 dis      -1.43        0.230    -6.24    1.17e- 9  1.43      Significant    \n 5 ptratio  -0.984       0.148    -6.64    1.08e-10  0.984     Significant    \n 6 lstat    -0.531       0.0551   -9.63    8.04e-20  0.531     Significant    \n 7 rad       0.303       0.0742    4.09    5.30e- 5  0.303     Significant    \n 8 crim     -0.118       0.0337   -3.50    5.18e- 4  0.118     Significant    \n 9 zn        0.0353      0.0160    2.21    2.78e- 2  0.0353    Significant    \n10 tax      -0.0111      0.00423  -2.64    8.72e- 3  0.0111    Significant    \n11 black     0.00824     0.00308   2.68    7.68e- 3  0.00824   Significant    \n12 age      -0.00327     0.0148   -0.221   8.25e- 1  0.00327   Not significant\n13 indus    -0.0000893   0.0689   -0.00130 9.99e- 1  0.0000893 Not significant\n\n\nThen, I did a visual comparison graph that is also shown in the section 4 (More Graphs and Analysis).\n\n\nCode\nlibrary(plotly)\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:MASS':\n\n    select\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n\nCode\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(tidyverse)\n\nshap_values_df &lt;- as.data.frame(shap_values)\n\nshap_importance &lt;- colMeans(abs(shap_values_df))\n\nimportance_df &lt;- data.frame(\n  Feature = names(shap_importance),\n  Importance = shap_importance\n)\n\nshap_df &lt;- data.frame(\n  Feature = names(shap_importance),\n  Importance = shap_importance,\n  Model = \"Random Forest (SHAP)\"\n)\n\nlibrary(broom)\n\n\nlm_model &lt;- lm(medv ~ ., data = train_data)\n\nlm_coef &lt;- tidy(lm_model) %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  mutate(\n    Feature = term,\n    Importance = abs(estimate),\n    Model = \"Linear Regression\"\n  ) %&gt;%\n  select(Feature, Importance, Model)\n\ncombined_df &lt;- bind_rows(shap_df, lm_coef)\n\nlibrary(ggplot2)\n\nggplot(combined_df, aes(x = reorder(Feature, Importance), y = Importance, fill = Model)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  coord_flip() +\n  labs(\n    title = \"Feature Importance: SHAP vs Linear Regression\",\n    x = \"Feature\",\n    y = \"Importance (Mean SHAP)\",\n    fill = \"Model\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nFrom the comparison graph, I can easily detect that SHAP highlights some important features that might not count as important in the linear regression model. Similarly, linear regression has some features that are not count important in the SHAP random forest model. The major differences could be due to the fact that linear regression model assumes linearity, which only captures additive, proportional relationships between the features and the target. But SHAP for random forest model captures the non-linearity and interactions. Random forest splits the data based on thresholds and combinations of features.\nHowever, I can still detect that there are some alignments for multiple features in the linear regression model and SHAP’s random forest model. It is because some features have a consistent directional effect on the target variable, no matter modeled linearly or non-linearly.\nI took nox as an example in the graphs and analysis section, so more details in the difference will be explained there.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results (With Some Graphs)</span>"
    ]
  },
  {
    "objectID": "data.html#description",
    "href": "data.html#description",
    "title": "2  Data",
    "section": "3.1 Description",
    "text": "3.1 Description",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "d3graph.html",
    "href": "d3graph.html",
    "title": "4  More Graphs and Analysis",
    "section": "",
    "text": "4.0.1 Graphs for both linear model and SHAP random forest\n\n\n\n\n\n\n\n\nCode\nlibrary(fastshap)\nlibrary(shapviz)\nlibrary(ggplot2)\nlibrary(MASS)\nlibrary(randomForest)\n\n\nrandomForest 4.7-1.2\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\n\nCode\nlibrary(lime)\n\n\n\nAttaching package: 'lime'\n\n\nThe following object is masked from 'package:fastshap':\n\n    explain\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::combine()       masks randomForest::combine()\n✖ dplyr::explain()       masks lime::explain(), fastshap::explain()\n✖ dplyr::filter()        masks stats::filter()\n✖ dplyr::lag()           masks stats::lag()\n✖ randomForest::margin() masks ggplot2::margin()\n✖ dplyr::select()        masks MASS::select()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(caret)\n\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\nCode\ndata(\"Boston\")\nX &lt;- Boston %&gt;% select(-medv)\ny &lt;- Boston$medv\n\n\n\n\nCode\ndata(\"Boston\")\n\nset.seed(42)\ntrain_index &lt;- createDataPartition(y, p = 0.8, list = FALSE)\nX_train &lt;- X[train_index, ]\ny_train &lt;- y[train_index]\nX_test &lt;- X[-train_index, ]\ny_test &lt;- y[-train_index]\n\ntrain_idx &lt;- sample(1:nrow(Boston), 0.8 * nrow(Boston))\ntrain_data &lt;- Boston[train_idx, ]\ntest_data &lt;- Boston[-train_idx, ]\n\nrf_model &lt;- randomForest(X_train, y_train)\nrf_model\n\n\n\nCall:\n randomForest(x = X_train, y = y_train) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 4\n\n          Mean of squared residuals: 11.26525\n                    % Var explained: 86.21\n\n\nCode\nlm_model &lt;- lm(y_train ~ ., data = X_train)\nlm_model\n\n\n\nCall:\nlm(formula = y_train ~ ., data = X_train)\n\nCoefficients:\n(Intercept)         crim           zn        indus         chas          nox  \n  33.998764    -0.122347     0.043296     0.003389     1.554178   -15.555219  \n         rm          age          dis          rad          tax      ptratio  \n   3.859758     0.004788    -1.305775     0.319482    -0.012468    -0.953531  \n      black        lstat  \n   0.009516    -0.516037  \n\n\nCode\nlm_importance &lt;- abs(coef(lm_model)[-1])\nnames(lm_importance) &lt;- names(X_train)\n\nrf_model &lt;- randomForest(medv ~ ., data = train_data)\n\nlm_model &lt;- lm(medv ~ ., data = train_data)\n\npredict_model_rf &lt;- function(model, newdata) {\n  predict(model, newdata)\n}\n\n\n\n\nCode\npredict_rf &lt;- function(model, newdata) {\n  predict(model, newdata = newdata)\n}\n\nshap_values &lt;- fastshap::explain(\n  rf_model,\n  X = test_data[1:5, -which(names(test_data) == \"medv\")],  \n  pred_wrapper = predict_rf,\n  nsim = 100\n)\n\ndim(shap_values) \n\n\n[1]  5 13\n\n\nCode\ndim(test_data[1:5, -which(names(test_data) == \"medv\")]) \n\n\n[1]  5 13\n\n\nCode\ncases_to_plot &lt;- 1:5\nshp &lt;- shapviz(\n  object = shap_values[cases_to_plot, ], \n  X = test_data[cases_to_plot, -which(names(test_data) == \"medv\")]\n)\n\n\nsv_force(shp, row_id = 1) \n\n\n\n\n\n\n\n\n\nThe sv_force graph is the force plot that is based on using SHAP model.\nIn this plot, features with negative values indicate that as they increases, the house price would decrease:\nlstat-Higher poverty→ Lowers home price\nHomes in low-income areas are valued less.\ncrim-Higher crime rate → Lowers home price\nSafety concerns might reduce buyer demand.\nnox-More pollution → Lowers home price\nAir quality negatively impacts the house price.\nage-Older property → Slightly lowers price\nAging infrastructure reduces the houses appeal.\nptratio-Pupil-Teacher Ratio → Increases price\nBetter school quality increases the location value\n\n\nCode\nsv_importance(shp)+\n   labs(title = \"Boston Housing: SHAP Feature Importance\",\n       x = \"Impact on Home Value Price\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nHere, I also made a feature importance graph using SHAP.\nFrom this graph with all the yellow bars, I can notice that: rm is the most influential one followed by lstat, indus, tax and so forth.\nrm is the most important feature: it has the widest spread of SHAP values, so it has a large influence on the model’s output.\nlstat is next in importance: its SHAP values tend to lower predictions, indicating areas with higher lstat typically have lower housing prices.\nindus and tax are also influential but less so…\n\n\nCode\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(broom)  \n\n\ndata(Boston)\n\n\nset.seed(42)\ntrain_idx &lt;- sample(1:nrow(Boston), 0.8 * nrow(Boston))\ntrain_data &lt;- Boston[train_idx, ]\ntest_data &lt;- Boston[-train_idx, ]\n\n\nlm_model &lt;- lm(medv ~ ., data = train_data)\n\nlm_coef &lt;- tidy(lm_model) %&gt;%\n  mutate(\n    abs_effect = abs(estimate), \n    significance = ifelse(p.value &lt; 0.05, \"Significant\", \"Not significant\")\n  )\n\n\nlm_coef %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  arrange(desc(abs_effect)) %&gt;%\n  head(5)\n\n\n# A tibble: 5 × 7\n  term    estimate std.error statistic  p.value abs_effect significance   \n  &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;          \n1 nox      -17.2       4.26      -4.03 6.77e- 5     17.2   Significant    \n2 rm         3.54      0.454      7.80 5.85e-14      3.54  Significant    \n3 chas       1.95      0.997      1.95 5.19e- 2      1.95  Not significant\n4 dis       -1.43      0.230     -6.24 1.17e- 9      1.43  Significant    \n5 ptratio   -0.984     0.148     -6.64 1.08e-10      0.984 Significant    \n\n\nFrom this linear regression model, I can notice that there are several significant variables that contribute to the model prediction, such as rm, rad, nox, dis, pupil-teacher ratio, lstat, and crim.\nIt is interesting to notice that the linear regression model also lists the feature effect that is not significant, which is chas in the Boston housing dataset.\nI think it shows that chas does not have a high feature effect in the linear model, but it doesn’t mean that chas is not important because feature importance usually does not consider p-values. Feature importance helps to know how the model relies on chas on average. In this case, chas is not significant means that it is not statistically significant.\n\n\nCode\nggplot(lm_coef %&gt;% filter(term != \"(Intercept)\"), \n       aes(x = reorder(term, estimate), y = estimate, fill = significance)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(\n    title = \"Linear Regression: Feature Effects on Home Value\",\n    x = \"Feature\",\n    y = \"Coefficient Estimate\",\n    fill = \"Significance\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(plotly)\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:MASS':\n\n    select\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n\nCode\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(tidyverse)\n\nshap_values_df &lt;- as.data.frame(shap_values)\n\nshap_importance &lt;- colMeans(abs(shap_values_df))\n\nimportance_df &lt;- data.frame(\n  Feature = names(shap_importance),\n  Importance = shap_importance\n)\n\nshap_df &lt;- data.frame(\n  Feature = names(shap_importance),\n  Importance = shap_importance,\n  Model = \"Random Forest (SHAP)\"\n)\n\nlibrary(broom)\n\n\nlm_model &lt;- lm(medv ~ ., data = train_data)\n\nlm_coef &lt;- tidy(lm_model) %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  mutate(\n    Feature = term,\n    Importance = abs(estimate),\n    Model = \"Linear Regression\"\n  ) %&gt;%\n  select(Feature, Importance, Model)\n\ncombined_df &lt;- bind_rows(shap_df, lm_coef)\n\nlibrary(ggplot2)\n\nggplot(combined_df, aes(x = reorder(Feature, Importance), y = Importance, fill = Model)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  coord_flip() +\n  labs(\n    title = \"Feature Importance: SHAP vs Linear Regression\",\n    x = \"Feature\",\n    y = \"Importance (Mean SHAP)\",\n    fill = \"Model\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nFrom the comparison graph, it is obvious that speaking of feature importance, linear regression model and the SHAP model highlights some consistent importance features for the boston housing dataset, which are rm, pupil-teacher ratio, lstat, and crim.\nBut linear regression model shows that nox is also important, and in fact, it is the most impactful feature in the model.\nIt’s interesting to notice that SHAP shows that it is slightly important to the overall model prediction. This pattern applies similarly for the feature chas.\nIn general, I will take nox as an example to talk about the fundamental difference in these two approaches.\nFor the Linear Regression model:\nnox shows high importance because linear assumes a linear relationship, and here it captures how strongly nox correlates with price (medv). Like if nox increases by 1 unit, price drops by a significant amount.\nRandom Forest + SHAP:\nnox has low importance because RF models non-linear and interaction effects, and here, nox’s predictive power might overlap with other features.\nSo, linear regression prioritizes nox as it correlates with price. It cannot assume the interactions. But random forest ignores nox because there are other features that better split the data, and nox’s effect might be captured indirectly through other variables.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>More Graphs and Analysis</span>"
    ]
  },
  {
    "objectID": "data.html#for-my-project-i-used-the-boston-housing-dataset-to-do-the-analysis.",
    "href": "data.html#for-my-project-i-used-the-boston-housing-dataset-to-do-the-analysis.",
    "title": "2  Data",
    "section": "",
    "text": "2.1.1 The following is the code using the random forest model for the Boston Housing Dataset!:\n\n\nCode\nlibrary(MASS)\nlibrary(randomForest)\n\n\nrandomForest 4.7-1.2\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\nCode\nlibrary(lime)\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::combine()  masks randomForest::combine()\n✖ dplyr::explain()  masks lime::explain()\n✖ dplyr::filter()   masks stats::filter()\n✖ dplyr::lag()      masks stats::lag()\n✖ ggplot2::margin() masks randomForest::margin()\n✖ dplyr::select()   masks MASS::select()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(caret)\n\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\nCode\ndata(\"Boston\")\nX &lt;- Boston %&gt;% select(-medv)\ny &lt;- Boston$medv\n\nset.seed(42)\ntrain_index &lt;- createDataPartition(y, p = 0.8, list = FALSE)\nX_train &lt;- X[train_index, ]\ny_train &lt;- y[train_index]\nX_test &lt;- X[-train_index, ]\ny_test &lt;- y[-train_index]\n\nrf_model &lt;- randomForest(X_train, y_train)\nrf_model\n\n\n\nCall:\n randomForest(x = X_train, y = y_train) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 4\n\n          Mean of squared residuals: 11.55371\n                    % Var explained: 85.85\n\n\n\n\n2.1.2 Here is code using the linear regression model for the Boston Housing Dataset!:\n\n\nCode\nlm_model &lt;- lm(y_train ~ ., data = X_train)\nlm_model\n\n\n\nCall:\nlm(formula = y_train ~ ., data = X_train)\n\nCoefficients:\n(Intercept)         crim           zn        indus         chas          nox  \n  33.998764    -0.122347     0.043296     0.003389     1.554178   -15.555219  \n         rm          age          dis          rad          tax      ptratio  \n   3.859758     0.004788    -1.305775     0.319482    -0.012468    -0.953531  \n      black        lstat  \n   0.009516    -0.516037  \n\n\nCode\nlm_importance &lt;- abs(coef(lm_model)[-1])\nnames(lm_importance) &lt;- names(X_train)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#i-will-use-both-the-random-forest-model-and-linear-regression-model-to-generate-the-feature-importance-explanations.",
    "href": "data.html#i-will-use-both-the-random-forest-model-and-linear-regression-model-to-generate-the-feature-importance-explanations.",
    "title": "2  Data",
    "section": "2.2 I will use both the random forest model and linear regression model to generate the feature importance explanations.",
    "text": "2.2 I will use both the random forest model and linear regression model to generate the feature importance explanations.\nThe code part provides the Random Forest model for the Boston Housing Dataset! 😊\nHere is code 1:\n\n\nCode\nlibrary(MASS)\nlibrary(randomForest)\n\n\nrandomForest 4.7-1.2\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\nCode\nlibrary(lime)\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::combine()  masks randomForest::combine()\n✖ dplyr::explain()  masks lime::explain()\n✖ dplyr::filter()   masks stats::filter()\n✖ dplyr::lag()      masks stats::lag()\n✖ ggplot2::margin() masks randomForest::margin()\n✖ dplyr::select()   masks MASS::select()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(caret)\n\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\nCode\ndata(\"Boston\")\nX &lt;- Boston %&gt;% select(-medv)\ny &lt;- Boston$medv\n\nset.seed(42)\ntrain_index &lt;- createDataPartition(y, p = 0.8, list = FALSE)\nX_train &lt;- X[train_index, ]\ny_train &lt;- y[train_index]\nX_test &lt;- X[-train_index, ]\ny_test &lt;- y[-train_index]\n\nrf_model &lt;- randomForest(X_train, y_train)\nrf_model\n\n\n\nCall:\n randomForest(x = X_train, y = y_train) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 4\n\n          Mean of squared residuals: 11.55371\n                    % Var explained: 85.85\n\n\n\n\nCode\nlm_model &lt;- lm(y_train ~ ., data = X_train)\nlm_model\n\n\n\nCall:\nlm(formula = y_train ~ ., data = X_train)\n\nCoefficients:\n(Intercept)         crim           zn        indus         chas          nox  \n  33.998764    -0.122347     0.043296     0.003389     1.554178   -15.555219  \n         rm          age          dis          rad          tax      ptratio  \n   3.859758     0.004788    -1.305775     0.319482    -0.012468    -0.953531  \n      black        lstat  \n   0.009516    -0.516037",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "5  Conclusion",
    "section": "",
    "text": "To answer the questions mentioned in the index.qmd:\nDo SHAP’s explanations about black box models align with the interpretability of intrinsically interpretable models like linear regression?\nI think some of the SHAP’s explanations do align with the interpretable model’s results. Specifcially, SHAP provides some valuable and consistent explanations with linear regression’s global feature importance, but what’s more, I think SHAP even also explores the non-linear effects that linear model misses to discover. It could show that SHAP is trustworthy and works as a complement to the linear regression model in understanding black-box predictions on the structured data like the Boston Housing dataset.\n1.Do the most important features identified by SHAP match those from linear regression?\nWell, I think there is a similar overlap in most of the top features identified by both SHAP-random forest and linear regression. Both models identify the important features like: -lstat -ptratio -rm -crim -rad\nBut the differences do emerge for some features, like there is a huge difference for the feature nox. Linear regression models detect nox as the most significant feature but SHAP for random forest model does not detect it as important at all.\n2.How consistent are SHAP’s explanations across similar observations?\nI think SHAP shows strong local consistency in some fields but I also noticed that in using k-nearest neighbors for some features like lstat, there is a high local inconsistency.\nThe local inconsistency may due to the interactions effects, like lstat and crim together could give a strong impact to the house price.\n3.Would SHAP and linear regression disagree on some parts?\nYes. For a few features in the Boston housing dataset, the two models show significant disagreement, which are: -nox -chas\nI think the difference is due to the fact that in the Boston Housing dataset, nox tends to be negatively correlated with medv. As pollution increases, housing prices drop. So the linear regression model sees it as strongly predictive.\nHowever, random forest captures the non-linear and interaction effects. So it could be the fact that nox’s signal overlaps with other features or nox affects the price only at extreme values.\nSHAP for random forest model only reflects how much each feature actually contributes to predictions across the trees, but not how strongly it correlates in isolation to the housing price, so it has a few inconsistent ideas with the linear regression model.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "index.html#research-question",
    "href": "index.html#research-question",
    "title": "Elinchen_project",
    "section": "1.1 Research Question:",
    "text": "1.1 Research Question:\nDo SHAP’s explanations about black box models align with the interpretability of intrinsically interpretable models like linear regression?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Topic</span>"
    ]
  },
  {
    "objectID": "index.html#key-elements-to-examine",
    "href": "index.html#key-elements-to-examine",
    "title": "Elinchen_project",
    "section": "1.2 Key elements to examine:",
    "text": "1.2 Key elements to examine:\n\nDo the most important features identified by SHAP match those from linear regression?\nHow consistent are SHAP’s explanations across similar observations?\nWould SHAP and linear regression disagree on some parts?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Topic</span>"
    ]
  }
]