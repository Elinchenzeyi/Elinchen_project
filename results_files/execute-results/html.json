{
  "hash": "3608b41b39760440485b516d4b2e46b8",
  "result": {
    "engine": "knitr",
    "markdown": "# Results (With Some Graphs)\n\nFrom the data section, I generated the train and test data, and the two different \nmodels are based on the data.\n\nI could tell that speaking of the model performance comparison:\nRandom Forest\t vs. Linear Regression\nMean Squared Residuals for random forest:\t11.5135.\nMean Squared Residuals for linear regression: tendency to be higher\n% Variance Explained for random forest:\t85.9%\t\n% Variance Explained for linear regression: tendency to be lower\nLinearity Assumed for random forest? No\nLinearity Assumed for linear regression?\tYes\t\nCaptures Interactions for random forest?\tYes\t\nCaptures Interactions for linear regression? No\nCaptures Non-linearity for random forest?\tYes\t\nCaptures Non-linearity for linear regression? No\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(randomForest)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrandomForest 4.7-1.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nType rfNews() to see new features/changes/bug fixes.\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(MASS)\nlibrary(randomForest)\nlibrary(lime)\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::combine()  masks randomForest::combine()\n✖ dplyr::explain()  masks lime::explain()\n✖ dplyr::filter()   masks stats::filter()\n✖ dplyr::lag()      masks stats::lag()\n✖ ggplot2::margin() masks randomForest::margin()\n✖ dplyr::select()   masks MASS::select()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(caret)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n```\n\n\n:::\n\n```{.r .cell-code}\ndata(\"Boston\")\nX <- Boston %>% select(-medv)\ny <- Boston$medv\n\nset.seed(42)\ntrain_index <- createDataPartition(y, p = 0.8, list = FALSE)\nX_train <- X[train_index, ]\ny_train <- y[train_index]\nX_test <- X[-train_index, ]\ny_test <- y[-train_index]\n\nrf_model <- randomForest(X_train, y_train, ntree = 500)\nlm_model <- lm(medv ~ ., data = Boston[train_index, ])\nrf_model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\n randomForest(x = X_train, y = y_train, ntree = 500) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 4\n\n          Mean of squared residuals: 11.55371\n                    % Var explained: 85.85\n```\n\n\n:::\n\n```{.r .cell-code}\nlm_model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = medv ~ ., data = Boston[train_index, ])\n\nCoefficients:\n(Intercept)         crim           zn        indus         chas          nox  \n  33.998764    -0.122347     0.043296     0.003389     1.554178   -15.555219  \n         rm          age          dis          rad          tax      ptratio  \n   3.859758     0.004788    -1.305775     0.319482    -0.012468    -0.953531  \n      black        lstat  \n   0.009516    -0.516037  \n```\n\n\n:::\n:::\n\n\n\n\n\nAfter knowing about the model differences, it is essential to check for \nconsistency in features for the SHAP random forest model for Boston\nhousing dataset. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(fastshap)  # For fast SHAP approximations\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'fastshap'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:dplyr':\n\n    explain\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:lime':\n\n    explain\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(randomForest)\nlibrary(dplyr)\nlibrary(FNN)\nlibrary(ggplot2)\n\ndata(Boston)\nX <- Boston %>% select(-medv)\ny <- Boston$medv\n\nset.seed(42)\ntrain_idx <- sample(seq_len(nrow(X)), size = 0.8 * nrow(X))\nX_train <- X[train_idx, ]\ny_train <- y[train_idx]\nX_test <- X[-train_idx, ]\ny_test <- y[-train_idx]\n\n# Train the Random Forest model\nrf_model <- randomForest(X_train, y_train, ntree = 500)\n\npred_fun <- function(model, newdata) predict(model, newdata = newdata)\n\nshap_values <- fastshap::explain(\n  rf_model,\n  X = X_train,\n  pred_wrapper = pred_fun,\n  nsim = 50\n)\n\nk <- 5\nknn_indices <- FNN::knnx.index(\n  data = X_train,\n  query = X_test,\n  k = k\n)\n\nshap_diffs <- sapply(1:nrow(X_test), function(i) {\n  test_shap <- shap_values[i, ]\n  knn_shap <- colMeans(shap_values[knn_indices[i, ], ])  \n  abs(test_shap - knn_shap)  \n})\n\navg_shap_diffs <- rowMeans(shap_diffs)\n\navg_shap_diffs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      crim         zn      indus       chas        nox         rm        age \n0.51302044 0.05223589 0.34801011 0.05687939 0.86705348 2.51544185 0.37886528 \n       dis        rad        tax    ptratio      black      lstat \n0.63195177 0.09928622 0.30506053 0.65355903 0.28250744 3.63036893 \n```\n\n\n:::\n:::\n\n\n\n\n\nWe could interpret from the data that the average SHAP differences is low\nfor the features like:\n-crim, -zn, -indus, -chas, -age, -dis, -rad, -tax, -ptratio, -black.\n\nHowever, there is a high average differences for the features:\n-rm, -lstat, -nox\n\nHere, I made a PDP and ICE plot for knowing the reasons like why some features\nwere not consistent: \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pdp)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'pdp'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:purrr':\n\n    partial\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\n# Partial Dependence Plot\npdp_lstat <- partial(rf_model, pred.var = \"lstat\", train = X_train)\nggplot(pdp_lstat, aes(lstat, yhat)) + \n  geom_line() +\n  labs(title = \"Partial Dependence of 'lstat' on Price\")\n```\n\n::: {.cell-output-display}\n![](results_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# ICE Plots\nice_lstat <- partial(rf_model, pred.var = \"lstat\", train = X_train, ice = TRUE)\nggplot(ice_lstat, aes(lstat, yhat, group = yhat.id)) + \n  geom_line(alpha = 0.2) +\n  labs(title = \"ICE Plots for 'lstat'\")\n```\n\n::: {.cell-output-display}\n![](results_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n:::\n\n\n\n\nFrom the graphs, I could notice that the trend of the curves show similar \npattern, which is that as the number of the lower percentage of the population \nincreases, the house price decreases significantly. \n\nThe pdp is non-linear and drops significantly within the 0-10 range for lstat,\nwhich implies that the house price is highly sensitive to lstat. \n\nSince the high inconsistency and non-linearity appear for the feature lstat,\nI made the graph for the rm+ lstat for knowing if lstat has an interaction with \nrm that causes the SHAP's explanation to be inconsistent. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(fastshap)\nshap_values <- explain(rf_model, X = X_train, nsim = 50, pred_wrapper = pred_fun)\n\nshap_values <- fastshap::explain(\n  rf_model, \n  X = X_train, \n  pred_wrapper = pred_fun, \n  nsim = 50\n)\n\nshap_df <- as.data.frame(shap_values)\nshap_df$lstat <- X_train$lstat  \nshap_df$crim <- X_train$crim    \n\nlibrary(ggplot2)\nggplot(shap_df, aes(lstat, lstat, color = crim)) +\n  geom_point(alpha = 0.5) +\n  scale_color_gradient(low = \"blue\", high = \"red\") +\n  labs(\n    title = \"SHAP Values for 'lstat' (Colored by Crime Rate)\",\n    x = \"lstat (Actual Value)\",\n    y = \"SHAP Value (Impact on Price)\"\n  )\n```\n\n::: {.cell-output-display}\n![](results_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\nI think from the graph above, it shows that the lstat and crim together give\na strong impact on the SHAP value (House price). Therefore, it explains for\nwhy there is a high inconsistency for the lstat feature itself. Similarly, \nI think the high inconsistency for nox and rm could also be influenced by the \ninteraction effects. \n\n\nThen, I explore the feature significance in linear regression model, and notice\nthat chas is the feature with no sigificance. I think it could tell that \nthe p-value for chas is larger than the standard threshold, 0.05. But again, I \nbelieve it is still an important feature, especially for understanding if it\nhas an interaction effect or it might has the coefficient value. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(fastshap)\nlibrary(shapviz)\nlibrary(ggplot2)\nlibrary(MASS)\nlibrary(randomForest)\nlibrary(lime)\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(broom)\ntrain_idx <- sample(1:nrow(Boston), 0.8 * nrow(Boston))\ntrain_data <- Boston[train_idx, ]\ntest_data <- Boston[-train_idx, ]\n\npredict_rf <- function(model, newdata) {\n  predict(model, newdata = newdata)\n}\n\nshap_values <- fastshap::explain(\n  rf_model,\n  X = test_data[1:5, -which(names(test_data) == \"medv\")],  \n  pred_wrapper = predict_rf,\n  nsim = 100\n)\n\ndim(shap_values) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  5 13\n```\n\n\n:::\n\n```{.r .cell-code}\ndim(test_data[1:5, -which(names(test_data) == \"medv\")]) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  5 13\n```\n\n\n:::\n\n```{.r .cell-code}\ncases_to_plot <- 1:5\nshp <- shapviz(\n  object = shap_values[cases_to_plot, ], \n  X = test_data[cases_to_plot, -which(names(test_data) == \"medv\")]\n)\n\n\n\ndata(Boston)\n\n\nset.seed(42)\ntrain_idx <- sample(1:nrow(Boston), 0.8 * nrow(Boston))\ntrain_data <- Boston[train_idx, ]\ntest_data <- Boston[-train_idx, ]\n\n\nlm_model <- lm(medv ~ ., data = train_data)\n\nlm_coef <- tidy(lm_model) %>%\n  mutate(\n    abs_effect = abs(estimate), \n    significance = ifelse(p.value < 0.05, \"Significant\", \"Not significant\")\n  )\n\n\nlm_coef %>%\n  filter(term != \"(Intercept)\") %>%\n  arrange(desc(abs_effect))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 13 × 7\n   term       estimate std.error statistic  p.value abs_effect significance   \n   <chr>         <dbl>     <dbl>     <dbl>    <dbl>      <dbl> <chr>          \n 1 nox     -17.2         4.26     -4.03    6.77e- 5 17.2       Significant    \n 2 rm        3.54        0.454     7.80    5.85e-14  3.54      Significant    \n 3 chas      1.95        0.997     1.95    5.19e- 2  1.95      Not significant\n 4 dis      -1.43        0.230    -6.24    1.17e- 9  1.43      Significant    \n 5 ptratio  -0.984       0.148    -6.64    1.08e-10  0.984     Significant    \n 6 lstat    -0.531       0.0551   -9.63    8.04e-20  0.531     Significant    \n 7 rad       0.303       0.0742    4.09    5.30e- 5  0.303     Significant    \n 8 crim     -0.118       0.0337   -3.50    5.18e- 4  0.118     Significant    \n 9 zn        0.0353      0.0160    2.21    2.78e- 2  0.0353    Significant    \n10 tax      -0.0111      0.00423  -2.64    8.72e- 3  0.0111    Significant    \n11 black     0.00824     0.00308   2.68    7.68e- 3  0.00824   Significant    \n12 age      -0.00327     0.0148   -0.221   8.25e- 1  0.00327   Not significant\n13 indus    -0.0000893   0.0689   -0.00130 9.99e- 1  0.0000893 Not significant\n```\n\n\n:::\n:::\n\n\n\n\n\nThen, I did a visual comparison graph that is also shown in the graphs.qmd. \nIt is the comparison graph that we can easily detect that SHAP highlights some \nimportant features that might not count as important in the linear regression\nmodel. Similarly, linear regression has some features that are not count\nimportant in the SHAP random forest model. \n\nHowever, there is still a strong alignment for multiple features in\nlinear regression model and SHAP's random forest model. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(plotly)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'plotly'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:MASS':\n\n    select\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:stats':\n\n    filter\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:graphics':\n\n    layout\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(tidyverse)\n\nshap_values_df <- as.data.frame(shap_values)\n\nshap_importance <- colMeans(abs(shap_values_df))\n\nimportance_df <- data.frame(\n  Feature = names(shap_importance),\n  Importance = shap_importance\n)\n\nshap_df <- data.frame(\n  Feature = names(shap_importance),\n  Importance = shap_importance,\n  Model = \"Random Forest (SHAP)\"\n)\n\nlibrary(broom)\n\n\nlm_model <- lm(medv ~ ., data = train_data)\n\nlm_coef <- tidy(lm_model) %>%\n  filter(term != \"(Intercept)\") %>%\n  mutate(\n    Feature = term,\n    Importance = abs(estimate),\n    Model = \"Linear Regression\"\n  ) %>%\n  select(Feature, Importance, Model)\n\ncombined_df <- bind_rows(shap_df, lm_coef)\n\nlibrary(ggplot2)\n\nggplot(combined_df, aes(x = reorder(Feature, Importance), y = Importance, fill = Model)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  coord_flip() +\n  labs(\n    title = \"Feature Importance: SHAP vs Linear Regression\",\n    x = \"Feature\",\n    y = \"Importance (Mean SHAP)\",\n    fill = \"Model\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](results_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "results_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}