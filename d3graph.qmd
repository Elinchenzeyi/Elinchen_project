# Interactive graph

<style>
  * {
    font-family: sans-serif;
  }
</style> 

<div id="plot">
</div>

<script src="https://cdn.jsdelivr.net/npm/d3@7"></script>
<script src="scripts/myscript.js"></script>


The following graph is the force plot that is based on using SHAP model.
In this plot, it shows the variables that affect the predictions the most. 
Specifically, I can notice that based on the predictions, rm is 6.01, which 
falls into the red region that shows decrease 0.225. I think it might suggest 
that average number of rooms puts the prediction down by 0.225.
However, the per capita crime rate, pupil-teacher ratio, lower status population,
proposition of old housing are the variables that increases our prediction.
Specifcially, I can notice that per capital crime rate is 0.0883, which increases
our model by 0.103. pupil teacher ratio is 15.2, which increases our model by
0.378. Age is 66.6, which increases our model by 0.587. 

E[f(x)] gives the base value across the training data. 

```{r}
library(fastshap)
library(shapviz)
library(ggplot2)
library(MASS)
library(randomForest)
library(lime)
library(tidyverse)
library(caret)

data("Boston")
X <- Boston %>% select(-medv)
y <- Boston$medv
```

```{r}
data("Boston")

set.seed(42)
train_index <- createDataPartition(y, p = 0.8, list = FALSE)
X_train <- X[train_index, ]
y_train <- y[train_index]
X_test <- X[-train_index, ]
y_test <- y[-train_index]

train_idx <- sample(1:nrow(Boston), 0.8 * nrow(Boston))
train_data <- Boston[train_idx, ]
test_data <- Boston[-train_idx, ]

rf_model <- randomForest(X_train, y_train)
rf_model

lm_model <- lm(y_train ~ ., data = X_train)
lm_model

lm_importance <- abs(coef(lm_model)[-1])
names(lm_importance) <- names(X_train)

rf_model <- randomForest(medv ~ ., data = train_data)

lm_model <- lm(medv ~ ., data = train_data)

predict_model_rf <- function(model, newdata) {
  predict(model, newdata)
}

```

```{r}
predict_rf <- function(model, newdata) {
  predict(model, newdata = newdata)
}

shap_values <- fastshap::explain(
  rf_model,
  X = test_data[1:5, -which(names(test_data) == "medv")],  
  pred_wrapper = predict_rf,
  nsim = 100
)

dim(shap_values) 
dim(test_data[1:5, -which(names(test_data) == "medv")]) 

cases_to_plot <- 1:5
shp <- shapviz(
  object = shap_values[cases_to_plot, ], 
  X = test_data[cases_to_plot, -which(names(test_data) == "medv")]
)


sv_force(shp, row_id = 1) 
  
```

Based on the sv_force graph, we can tell that the model's actual prediction is 
the formula:

Base Value + Sum of SHAP Contributions = 0 + (+0.378 +0.528 +0.587-0.225 + ...) 
=1.3
```{r}
function_x<-1.3
```


It shows the variables that are the most important ones in our SHAP feature graph
and from this graph with all the yellow bars, we can notice that lstat is the 
most influential one followed by age, and then pupil-teacher ratio, rm and so 
forth. 

```{r}
sv_importance(shp)+
   labs(title = "Boston Housing: SHAP Feature Importance",
       x = "Impact on Home Value Price") +
  theme_minimal()
```

Graphs with linear regression models: 
```{r}
library(MASS)
library(tidyverse)
library(broom)  # For tidy model summaries

# Load data
data(Boston)

# Train-test split (consistent with your RF model)
set.seed(42)
train_idx <- sample(1:nrow(Boston), 0.8 * nrow(Boston))
train_data <- Boston[train_idx, ]
test_data <- Boston[-train_idx, ]

# Fit linear regression
lm_model <- lm(medv ~ ., data = train_data)  # medv = median home value ($1000s)

lm_coef <- tidy(lm_model) %>%
  mutate(
    abs_effect = abs(estimate),  # Absolute effect size
    significance = ifelse(p.value < 0.05, "Significant", "Not significant")
  )

# Print top 5 most impactful features
lm_coef %>%
  filter(term != "(Intercept)") %>%
  arrange(desc(abs_effect)) %>%
  head(5)


```

From this linear regression model, I can notice that there are several 
significant variables that contribute to the model prediction, such as
the rm, rad, nox, dis, pupil-teacher ratio, lstat, crim. It is interesting 
to notice that the linear regression model also lists the variable that is not
significant, which is chas in the Boston Housing dataset. 

```{r}
ggplot(lm_coef %>% filter(term != "(Intercept)"), 
       aes(x = reorder(term, estimate), y = estimate, fill = significance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(
    title = "Linear Regression: Feature Effects on Home Value",
    x = "Feature",
    y = "Coefficient Estimate ($1000s)",
    fill = "Significance"
  ) +
  theme_minimal()
```


