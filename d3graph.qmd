# More Graphs and Analysis
### Graphs for both linear model and SHAP random forest 

<style>
  * {
    font-family: sans-serif;
  }
</style> 

<div id="plot">
</div>

<script src="https://cdn.jsdelivr.net/npm/d3@7"></script>
<script src="scripts/myscript.js"></script>



```{r}
library(fastshap)
library(shapviz)
library(ggplot2)
library(MASS)
library(randomForest)
library(lime)
library(tidyverse)
library(caret)

data("Boston")
X <- Boston %>% select(-medv)
y <- Boston$medv
```

```{r}
data("Boston")

set.seed(42)
train_index <- createDataPartition(y, p = 0.8, list = FALSE)
X_train <- X[train_index, ]
y_train <- y[train_index]
X_test <- X[-train_index, ]
y_test <- y[-train_index]

train_idx <- sample(1:nrow(Boston), 0.8 * nrow(Boston))
train_data <- Boston[train_idx, ]
test_data <- Boston[-train_idx, ]

rf_model <- randomForest(X_train, y_train)
rf_model

lm_model <- lm(y_train ~ ., data = X_train)
lm_model

lm_importance <- abs(coef(lm_model)[-1])
names(lm_importance) <- names(X_train)

rf_model <- randomForest(medv ~ ., data = train_data)

lm_model <- lm(medv ~ ., data = train_data)

predict_model_rf <- function(model, newdata) {
  predict(model, newdata)
}

```

```{r}
predict_rf <- function(model, newdata) {
  predict(model, newdata = newdata)
}

shap_values <- fastshap::explain(
  rf_model,
  X = test_data[1:5, -which(names(test_data) == "medv")],  
  pred_wrapper = predict_rf,
  nsim = 100
)

dim(shap_values) 
dim(test_data[1:5, -which(names(test_data) == "medv")]) 

cases_to_plot <- 1:5
shp <- shapviz(
  object = shap_values[cases_to_plot, ], 
  X = test_data[cases_to_plot, -which(names(test_data) == "medv")]
)


sv_force(shp, row_id = 1) 
  
```
The sv_force graph is the force plot that is based on using SHAP model.

In this plot, features with negative values indicate that as they increases, 
the house price would decrease: 

lstat-Higher poverty→ Lowers home price	

Homes in low-income areas are valued less.


crim-Higher crime rate → Lowers home price

Safety concerns might reduce buyer demand.


nox-More pollution → Lowers home price	

Air quality negatively impacts the house price.


age-Older property → Slightly lowers price	

Aging infrastructure reduces the houses appeal.


ptratio-Pupil-Teacher Ratio → Increases price

Better school quality increases the location value



```{r}
sv_importance(shp)+
   labs(title = "Boston Housing: SHAP Feature Importance",
       x = "Impact on Home Value Price") +
  theme_minimal()
```

Here, I also made a feature importance graph using SHAP. 

From this graph with all the yellow bars, I can notice that: rm is the 
most influential one followed by lstat, indus, tax and so forth. 

rm is the most important feature: it has the widest spread of SHAP values, 
so it has a large influence on the model’s output.

lstat is next in importance: its SHAP values tend to lower predictions, 
indicating areas with higher lstat typically have lower housing prices.

indus and tax are also influential but less so... 



```{r}
library(MASS)
library(tidyverse)
library(broom)  


data(Boston)


set.seed(42)
train_idx <- sample(1:nrow(Boston), 0.8 * nrow(Boston))
train_data <- Boston[train_idx, ]
test_data <- Boston[-train_idx, ]


lm_model <- lm(medv ~ ., data = train_data)

lm_coef <- tidy(lm_model) %>%
  mutate(
    abs_effect = abs(estimate), 
    significance = ifelse(p.value < 0.05, "Significant", "Not significant")
  )


lm_coef %>%
  filter(term != "(Intercept)") %>%
  arrange(desc(abs_effect)) %>%
  head(5)


```

From this linear regression model, 
I can notice that there are several significant variables that contribute to 
the model prediction, such as rm, rad, nox, dis, pupil-teacher ratio, lstat, 
and crim. 


It is interesting to notice that the linear regression model also lists the 
feature effect that is not significant, which is chas in the Boston housing 
dataset. 


I think it shows that chas does not have a high feature effect in the linear
model, but it doesn't mean that chas is not important because feature importance
usually does not consider p-values. Feature importance helps to know how the 
model relies on chas on average. In this case, chas is not significant means 
that it is not statistically significant. 



```{r}
ggplot(lm_coef %>% filter(term != "(Intercept)"), 
       aes(x = reorder(term, estimate), y = estimate, fill = significance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(
    title = "Linear Regression: Feature Effects on Home Value",
    x = "Feature",
    y = "Coefficient Estimate",
    fill = "Significance"
  ) +
  theme_minimal()
```


```{r}
library(plotly)
library(shiny)
library(ggplot2)
library(tidyverse)

shap_values_df <- as.data.frame(shap_values)

shap_importance <- colMeans(abs(shap_values_df))

importance_df <- data.frame(
  Feature = names(shap_importance),
  Importance = shap_importance
)

shap_df <- data.frame(
  Feature = names(shap_importance),
  Importance = shap_importance,
  Model = "Random Forest (SHAP)"
)

library(broom)


lm_model <- lm(medv ~ ., data = train_data)

lm_coef <- tidy(lm_model) %>%
  filter(term != "(Intercept)") %>%
  mutate(
    Feature = term,
    Importance = abs(estimate),
    Model = "Linear Regression"
  ) %>%
  select(Feature, Importance, Model)

combined_df <- bind_rows(shap_df, lm_coef)

library(ggplot2)

ggplot(combined_df, aes(x = reorder(Feature, Importance), y = Importance, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  coord_flip() +
  labs(
    title = "Feature Importance: SHAP vs Linear Regression",
    x = "Feature",
    y = "Importance (Mean SHAP)",
    fill = "Model"
  ) +
  theme_minimal()



```

From the comparison graph, it is obvious that speaking of feature
importance, linear regression model and the SHAP model highlights some
consistent importance features for the boston housing dataset, which are 
rm, pupil-teacher ratio, lstat, and crim. 


But linear regression model shows that nox is also important, and in fact, 
it is the most impactful feature in the model. 


It's interesting to notice that SHAP shows that it is slightly important to the
overall model prediction.
This pattern applies similarly for the feature chas. 


In general, I will take nox as an example to talk about the fundamental 
difference in these two approaches.

For the Linear Regression model:

nox shows high importance because linear assumes a linear relationship, and here
it captures how strongly nox correlates with price (medv).
Like if nox increases by 1 unit, price drops by a significant amount.

Random Forest + SHAP:

nox has low importance because RF models non-linear and interaction effects,
and here, nox’s predictive power might overlap with other features.

So, linear regression prioritizes nox as it correlates with price. It cannot 
assume the interactions. But random forest ignores nox because there are 
other features that better split the data, and nox’s effect might be captured 
indirectly through other variables.

