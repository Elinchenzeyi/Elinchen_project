# Graphs for both linear model and SHAP random forest 

<style>
  * {
    font-family: sans-serif;
  }
</style> 

<div id="plot">
</div>

<script src="https://cdn.jsdelivr.net/npm/d3@7"></script>
<script src="scripts/myscript.js"></script>


The following graph is the force plot that is based on using SHAP model.
In this plot, it shows the variables that affect the predictions the most. 
Specifically, I can notice that based on the predictions, rm is 6.01, which 
falls into the red region that shows decrease 0.225. I think it might suggest 
that average number of rooms puts the prediction down by 0.225. But it does
not mean that this feature is not important. Instead, this feature is important
as it creates impact for our model.

Also, we can notice that the per capita crime rate, pupil-teacher ratio, 
lower status population, proposition of old housing are the variables that 
increases our prediction.

Specifcially, I can notice that per capital crime rate is 0.0883, which increases
our model by 0.103. pupil teacher ratio is 15.2, which increases our model by
0.378. Age is 66.6, which increases our model by 0.587. 

Here, I think the E[f(x)] gives the base value across the training data. 

```{r}
library(fastshap)
library(shapviz)
library(ggplot2)
library(MASS)
library(randomForest)
library(lime)
library(tidyverse)
library(caret)

data("Boston")
X <- Boston %>% select(-medv)
y <- Boston$medv
```

```{r}
data("Boston")

set.seed(42)
train_index <- createDataPartition(y, p = 0.8, list = FALSE)
X_train <- X[train_index, ]
y_train <- y[train_index]
X_test <- X[-train_index, ]
y_test <- y[-train_index]

train_idx <- sample(1:nrow(Boston), 0.8 * nrow(Boston))
train_data <- Boston[train_idx, ]
test_data <- Boston[-train_idx, ]

rf_model <- randomForest(X_train, y_train)
rf_model

lm_model <- lm(y_train ~ ., data = X_train)
lm_model

lm_importance <- abs(coef(lm_model)[-1])
names(lm_importance) <- names(X_train)

rf_model <- randomForest(medv ~ ., data = train_data)

lm_model <- lm(medv ~ ., data = train_data)

predict_model_rf <- function(model, newdata) {
  predict(model, newdata)
}

```

```{r}
predict_rf <- function(model, newdata) {
  predict(model, newdata = newdata)
}

shap_values <- fastshap::explain(
  rf_model,
  X = test_data[1:5, -which(names(test_data) == "medv")],  
  pred_wrapper = predict_rf,
  nsim = 100
)

dim(shap_values) 
dim(test_data[1:5, -which(names(test_data) == "medv")]) 

cases_to_plot <- 1:5
shp <- shapviz(
  object = shap_values[cases_to_plot, ], 
  X = test_data[cases_to_plot, -which(names(test_data) == "medv")]
)


sv_force(shp, row_id = 1) 
  
```

Based on the sv_force graph, we can tell that the model's actual prediction is 
the formula:

Base Value + Sum of SHAP Contributions = 0 + (+0.378 +0.528 +0.587-0.225 + ...) 
=1.3
```{r}
function_x<-1.3
```


It shows the variables that are the most important ones in our SHAP feature graph
and from this graph with all the yellow bars, we can notice that lstat is the 
most influential one followed by age, and then pupil-teacher ratio, rm and so 
forth. 

```{r}
sv_importance(shp)+
   labs(title = "Boston Housing: SHAP Feature Importance",
       x = "Impact on Home Value Price") +
  theme_minimal()
```

Graphs with linear regression models: 
```{r}
library(MASS)
library(tidyverse)
library(broom)  


data(Boston)


set.seed(42)
train_idx <- sample(1:nrow(Boston), 0.8 * nrow(Boston))
train_data <- Boston[train_idx, ]
test_data <- Boston[-train_idx, ]


lm_model <- lm(medv ~ ., data = train_data)

lm_coef <- tidy(lm_model) %>%
  mutate(
    abs_effect = abs(estimate), 
    significance = ifelse(p.value < 0.05, "Significant", "Not significant")
  )


lm_coef %>%
  filter(term != "(Intercept)") %>%
  arrange(desc(abs_effect)) %>%
  head(5)


```

From this linear regression model, I can notice that there are several 
significant variables that contribute to the model prediction, such as
the rm, rad, nox, dis, pupil-teacher ratio, lstat, crim. It is interesting 
to notice that the linear regression model also lists the feature effect that
is not significant, which is chas in the Boston Housing dataset. 

I think it shows that chas is not statistically significant in the linear
model, but it doesn't mean that chas is not important because feature importance
usually does not consider p-values. Feature importance helps to know how the 
model relies on chas on average. 

```{r}
ggplot(lm_coef %>% filter(term != "(Intercept)"), 
       aes(x = reorder(term, estimate), y = estimate, fill = significance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(
    title = "Linear Regression: Feature Effects on Home Value",
    x = "Feature",
    y = "Coefficient Estimate",
    fill = "Significance"
  ) +
  theme_minimal()
```


From the comparison graph I made, it is obvious that speaking of feature
importance, linear regression model and the SHAP model highlights some
consistent importance features for the boston housing dataset, which are 
rm, pupil-teacher ratio, lstat, and crim. 

But linear regression model shows that nox is also important, and in fact, 
it is the most impactful feature in the model. It's interesting to notice
that SHAP shows that it is slightly important to the overall model prediction.
This pattern applies similarly for the feature chas. 

```{r}
library(plotly)
library(shiny)
library(ggplot2)
library(tidyverse)

shap_values_df <- as.data.frame(shap_values)

shap_importance <- colMeans(abs(shap_values_df))

importance_df <- data.frame(
  Feature = names(shap_importance),
  Importance = shap_importance
)

shap_df <- data.frame(
  Feature = names(shap_importance),
  Importance = shap_importance,
  Model = "Random Forest (SHAP)"
)

library(broom)


lm_model <- lm(medv ~ ., data = train_data)

lm_coef <- tidy(lm_model) %>%
  filter(term != "(Intercept)") %>%
  mutate(
    Feature = term,
    Importance = abs(estimate),
    Model = "Linear Regression"
  ) %>%
  select(Feature, Importance, Model)

combined_df <- bind_rows(shap_df, lm_coef)

library(ggplot2)

ggplot(combined_df, aes(x = reorder(Feature, Importance), y = Importance, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  coord_flip() +
  labs(
    title = "Feature Importance: SHAP vs Linear Regression",
    x = "Feature",
    y = "Importance (Mean SHAP)",
    fill = "Model"
  ) +
  theme_minimal()



```

